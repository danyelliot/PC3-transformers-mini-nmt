{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705dd574",
   "metadata": {},
   "source": [
    "# Notebook 1: Fundamentos Teóricos del Transformer\n",
    "\n",
    "Este notebook explica los componentes fundamentales de la arquitectura Transformer implementada para traducción automática neuronal (NMT) español-inglés.\n",
    "\n",
    "## Contenido\n",
    "1. Arquitectura Encoder-Decoder\n",
    "2. Mecanismo de Atención\n",
    "3. Máscaras de Atención\n",
    "4. Codificaciones Posicionales\n",
    "5. Estrategias de Decodificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88c643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from src.models.attention import ScaledDotProductAttention, create_causal_mask, create_padding_mask\n",
    "from src.models.posenc import SinusoidalPositionalEncoding, RotaryPositionalEmbedding, ALiBiPositionalBias\n",
    "from src.models.mhsa import MultiHeadAttention\n",
    "from src.models.transformer import TransformerConfig, Transformer\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b271e",
   "metadata": {},
   "source": [
    "## 1. Arquitectura Encoder-Decoder\n",
    "\n",
    "El Transformer utiliza una arquitectura encoder-decoder donde:\n",
    "\n",
    "**Encoder:**\n",
    "- Procesa la secuencia source (español) de forma bidireccional\n",
    "- Cada token puede atender a todos los demás tokens\n",
    "- Genera representaciones contextuales ricas\n",
    "\n",
    "**Decoder:**\n",
    "- Genera la secuencia target (inglés) de forma autoregresiva\n",
    "- Self-attention con máscara causal (solo ve tokens pasados)\n",
    "- Cross-attention al encoder para incorporar información del source\n",
    "\n",
    "### Configuración del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad329035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo\n",
    "config = TransformerConfig(\n",
    "    vocab_size_src=10000,\n",
    "    vocab_size_tgt=10000,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=100,\n",
    "    pos_encoding_type='sinusoidal'\n",
    ")\n",
    "\n",
    "print(f\"Configuración del Transformer:\")\n",
    "print(f\"  - d_model: {config.d_model}\")\n",
    "print(f\"  - n_heads: {config.n_heads}\")\n",
    "print(f\"  - encoder_layers: {config.num_encoder_layers}\")\n",
    "print(f\"  - decoder_layers: {config.num_decoder_layers}\")\n",
    "print(f\"  - dim_feedforward: {config.dim_feedforward}\")\n",
    "print(f\"  - pos_encoding: {config.pos_encoding_type}\")\n",
    "\n",
    "# Instanciar modelo\n",
    "model = Transformer(config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParámetros del modelo:\")\n",
    "print(f\"  - Total: {total_params:,}\")\n",
    "print(f\"  - Entrenables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02916c",
   "metadata": {},
   "source": [
    "## 2. Mecanismo de Atención\n",
    "\n",
    "La atención escalada de producto punto (Scaled Dot-Product Attention) es el componente fundamental:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $Q$ (Query): representa \"qué busco\"\n",
    "- $K$ (Key): representa \"qué ofrezco\"\n",
    "- $V$ (Value): representa \"qué información tengo\"\n",
    "- $d_k$: dimensión de las keys (para estabilizar gradientes)\n",
    "\n",
    "### Visualización de Atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b88f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo\n",
    "batch_size, seq_len, d_model = 1, 8, 64\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Calcular atención\n",
    "attention = ScaledDotProductAttention(dropout=0.0)\n",
    "output, attn_weights = attention(Q, K, V)\n",
    "\n",
    "# Visualizar matriz de atención\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights[0].detach().numpy(), \n",
    "            cmap='viridis', \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Peso de atención'})\n",
    "plt.xlabel('Posición Key/Value')\n",
    "plt.ylabel('Posición Query')\n",
    "plt.title('Matriz de Atención (sin máscara)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificar que los pesos suman 1\n",
    "print(f\"Suma de pesos por fila (debería ser ~1.0):\")\n",
    "print(attn_weights[0].sum(dim=-1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a3d1fd",
   "metadata": {},
   "source": [
    "## 3. Máscaras de Atención\n",
    "\n",
    "Las máscaras controlan qué tokens pueden atender a cuáles:\n",
    "\n",
    "### 3.1 Máscara Causal (Decoder)\n",
    "\n",
    "Evita que el modelo vea tokens futuros durante el entrenamiento:\n",
    "- Token en posición $i$ solo puede atender a posiciones $j \\leq i$\n",
    "- Implementada como matriz triangular inferior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear máscara causal\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Máscara causal\n",
    "sns.heatmap(causal_mask.squeeze().numpy(), \n",
    "            cmap='RdYlGn', \n",
    "            annot=True, \n",
    "            fmt='d',\n",
    "            cbar_kws={'label': 'Permitido'},\n",
    "            ax=axes[0])\n",
    "axes[0].set_xlabel('Posición Key')\n",
    "axes[0].set_ylabel('Posición Query')\n",
    "axes[0].set_title('Máscara Causal\\n(True=permitido, False=bloqueado)')\n",
    "\n",
    "# Aplicar máscara a atención\n",
    "output_masked, attn_weights_masked = attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "sns.heatmap(attn_weights_masked[0].detach().numpy(), \n",
    "            cmap='viridis', \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Peso de atención'},\n",
    "            ax=axes[1])\n",
    "axes[1].set_xlabel('Posición Key/Value')\n",
    "axes[1].set_ylabel('Posición Query')\n",
    "axes[1].set_title('Atención con Máscara Causal\\n(triángulo superior = 0)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Verificación: pesos en triángulo superior (futuro) deberían ser ~0\")\n",
    "print(f\"Peso [0,7] (futuro): {attn_weights_masked[0, 0, 7].item():.6f}\")\n",
    "print(f\"Peso [7,0] (pasado): {attn_weights_masked[0, 7, 0].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc2583",
   "metadata": {},
   "source": [
    "### 3.2 Máscara de Padding\n",
    "\n",
    "Bloquea tokens de padding (PAD) para que no contribuyan a la atención:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ef468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secuencia con padding (0 = PAD)\n",
    "seq_with_pad = torch.tensor([[1, 2, 3, 4, 5, 0, 0, 0]])  # 5 tokens reales, 3 PAD\n",
    "pad_mask = create_padding_mask(seq_with_pad, pad_idx=0)\n",
    "\n",
    "print(f\"Secuencia: {seq_with_pad[0].tolist()}\")\n",
    "print(f\"Máscara de padding (True=válido):\")\n",
    "print(pad_mask[0, 0].numpy())\n",
    "\n",
    "# Visualizar efecto\n",
    "output_padded, attn_weights_padded = attention(Q, K, V, mask=pad_mask)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attn_weights_padded[0].detach().numpy(), \n",
    "            cmap='viridis', \n",
    "            annot=True, \n",
    "            fmt='.2f',\n",
    "            cbar_kws={'label': 'Peso de atención'})\n",
    "plt.xlabel('Posición Key/Value')\n",
    "plt.ylabel('Posición Query')\n",
    "plt.title('Atención con Máscara de Padding\\n(columnas 5-7 deberían ser ~0)')\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=2, label='Inicio padding')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSuma de atención sobre tokens de padding (debería ser ~0):\")\n",
    "print(f\"Suma columnas 5-7: {attn_weights_padded[0, :, 5:].sum().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707970d",
   "metadata": {},
   "source": [
    "## 4. Codificaciones Posicionales\n",
    "\n",
    "Las codificaciones posicionales añaden información de orden a las secuencias.\n",
    "\n",
    "### 4.1 Sinusoidal (Original)\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear codificación sinusoidal\n",
    "max_len = 100\n",
    "d_model = 128\n",
    "sinusoidal_pe = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Extraer las codificaciones\n",
    "pe_matrix = sinusoidal_pe.pe.squeeze(0).numpy()  # (max_len, d_model)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Heatmap completo\n",
    "im = axes[0].imshow(pe_matrix.T, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
    "axes[0].set_xlabel('Posición')\n",
    "axes[0].set_ylabel('Dimensión')\n",
    "axes[0].set_title('Codificación Posicional Sinusoidal (Heatmap)')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Algunas dimensiones específicas\n",
    "dims_to_plot = [0, 1, 2, 3, 64, 65]\n",
    "for dim in dims_to_plot:\n",
    "    axes[1].plot(pe_matrix[:, dim], label=f'dim {dim}', alpha=0.7)\n",
    "axes[1].set_xlabel('Posición')\n",
    "axes[1].set_ylabel('Valor de codificación')\n",
    "axes[1].set_title('Codificación Posicional por Dimensión')\n",
    "axes[1].legend(ncol=3)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Propiedades de la codificación sinusoidal:\")\n",
    "print(f\"  - Dimensiones pares usan seno\")\n",
    "print(f\"  - Dimensiones impares usan coseno\")\n",
    "print(f\"  - Frecuencias más bajas para dimensiones altas\")\n",
    "print(f\"  - Permite extrapolación a secuencias más largas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4905128",
   "metadata": {},
   "source": [
    "### 4.2 RoPE (Rotary Position Embedding)\n",
    "\n",
    "En lugar de sumar, RoPE rota las representaciones Q y K según la posición:\n",
    "\n",
    "$$\n",
    "q_m = R_m q, \\quad k_n = R_n k\n",
    "$$\n",
    "\n",
    "donde $R_i$ es una matriz de rotación dependiente de la posición.\n",
    "\n",
    "**Propiedad clave:** Preserva la norma de los vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad806d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear RoPE\n",
    "head_dim = 64\n",
    "rope = RotaryPositionalEmbedding(head_dim, max_seq_len=100)\n",
    "\n",
    "# Crear queries y keys de prueba\n",
    "batch, n_heads, seq_len, dim = 1, 8, 50, head_dim\n",
    "q = torch.randn(batch, n_heads, seq_len, dim)\n",
    "k = torch.randn(batch, n_heads, seq_len, dim)\n",
    "\n",
    "# Aplicar RoPE\n",
    "q_rope, k_rope = rope(q, k)\n",
    "\n",
    "# Verificar preservación de norma\n",
    "q_norm_before = torch.norm(q, dim=-1)\n",
    "q_norm_after = torch.norm(q_rope, dim=-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Comparar normas\n",
    "axes[0].plot(q_norm_before[0, 0].numpy(), label='Antes de RoPE', marker='o', markersize=3)\n",
    "axes[0].plot(q_norm_after[0, 0].numpy(), label='Después de RoPE', marker='x', markersize=3)\n",
    "axes[0].set_xlabel('Posición')\n",
    "axes[0].set_ylabel('Norma L2')\n",
    "axes[0].set_title('Preservación de Norma en RoPE')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diferencia absoluta\n",
    "diff = torch.abs(q_norm_before - q_norm_after)[0, 0].numpy()\n",
    "axes[1].plot(diff, marker='o', markersize=3, color='red')\n",
    "axes[1].set_xlabel('Posición')\n",
    "axes[1].set_ylabel('Diferencia absoluta de norma')\n",
    "axes[1].set_title('Error en Preservación de Norma')\n",
    "axes[1].axhline(y=1e-5, color='green', linestyle='--', label='Tolerancia (1e-5)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Máxima diferencia en norma: {diff.max():.2e}\")\n",
    "print(f\"Test de preservación: {'PASS' if diff.max() < 1e-5 else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2101d",
   "metadata": {},
   "source": [
    "### 4.3 ALiBi (Attention with Linear Biases)\n",
    "\n",
    "ALiBi añade un sesgo lineal a las puntuaciones de atención:\n",
    "\n",
    "$$\n",
    "\\text{score}_{i,j} = q_i \\cdot k_j - m \\cdot |i - j|\n",
    "$$\n",
    "\n",
    "donde $m$ es una pendiente específica de cada cabeza de atención.\n",
    "\n",
    "**Ventaja:** Excelente extrapolación a secuencias más largas sin entrenamiento adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d45a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear ALiBi\n",
    "n_heads = 8\n",
    "alibi = ALiBiPositionalBias(n_heads, max_seq_len=100)\n",
    "\n",
    "# Generar sesgos para diferentes longitudes\n",
    "seq_lens = [10, 20, 50]\n",
    "fig, axes = plt.subplots(1, len(seq_lens), figsize=(15, 4))\n",
    "\n",
    "for idx, seq_len in enumerate(seq_lens):\n",
    "    bias = alibi(seq_len)  # (1, n_heads, seq_len, seq_len)\n",
    "    \n",
    "    # Visualizar primera cabeza\n",
    "    im = axes[idx].imshow(bias[0, 0].numpy(), cmap='RdYlBu_r', aspect='auto')\n",
    "    axes[idx].set_xlabel('Posición Key')\n",
    "    axes[idx].set_ylabel('Posición Query')\n",
    "    axes[idx].set_title(f'ALiBi (seq_len={seq_len})\\nCabeza 1')\n",
    "    plt.colorbar(im, ax=axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Verificar monotonicidad (debe decrecer con la distancia)\n",
    "bias_50 = alibi(50)\n",
    "print(\"\\nVerificación de monotonicidad (fila 0, cabeza 0):\")\n",
    "print(\"Valores para posiciones consecutivas (deberían decrecer):\")\n",
    "for i in range(5):\n",
    "    print(f\"  Posición {i}: {bias_50[0, 0, 0, i].item():.4f}\")\n",
    "\n",
    "# Pendientes por cabeza\n",
    "print(\"\\nPendientes por cabeza (m):\")\n",
    "slopes = alibi._get_slopes(n_heads)\n",
    "for i, slope in enumerate(slopes):\n",
    "    print(f\"  Cabeza {i}: {slope:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90908d",
   "metadata": {},
   "source": [
    "### Comparación de Codificaciones Posicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5af217",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    'Método': ['Sinusoidal', 'RoPE', 'ALiBi'],\n",
    "    'Parámetros adicionales': ['No', 'No', 'No'],\n",
    "    'Extrapolación': ['Limitada', 'Buena', 'Excelente'],\n",
    "    'Complejidad': ['Baja', 'Media', 'Baja'],\n",
    "    'Preserva norma': ['No aplica', 'Sí', 'No aplica'],\n",
    "    'Información': ['Absoluta', 'Relativa', 'Relativa']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nComparación de Métodos de Codificación Posicional:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDACIONES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"- Sinusoidal: Baseline estándar, bueno para secuencias dentro del rango de entrenamiento\")\n",
    "print(\"- RoPE: Mejor para capturar relaciones posicionales, popular en modelos modernos (LLaMA, GPT-NeoX)\")\n",
    "print(\"- ALiBi: Ideal si necesitas inferencia en secuencias más largas que las de entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da994bf4",
   "metadata": {},
   "source": [
    "## 5. Estrategias de Decodificación\n",
    "\n",
    "Durante la inferencia, existen varias estrategias para generar secuencias:\n",
    "\n",
    "### 5.1 Greedy Search\n",
    "\n",
    "Selecciona el token más probable en cada paso:\n",
    "\n",
    "$$\n",
    "y_t = \\arg\\max P(y_t | y_{<t}, x)\n",
    "$$\n",
    "\n",
    "**Ventajas:** Rápido, determinista\n",
    "\n",
    "**Desventajas:** Puede quedar atrapado en óptimos locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac36f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de greedy search\n",
    "vocab_size = 100\n",
    "seq_len = 10\n",
    "\n",
    "# Generar distribuciones de probabilidad simuladas\n",
    "logits = torch.randn(seq_len, vocab_size)\n",
    "probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "# Greedy: seleccionar argmax\n",
    "greedy_tokens = torch.argmax(probs, dim=-1)\n",
    "\n",
    "print(\"Ejemplo de Greedy Search:\")\n",
    "print(f\"Tokens seleccionados: {greedy_tokens.tolist()}\")\n",
    "print(f\"\\nProbabilidades de los tokens seleccionados:\")\n",
    "for i in range(seq_len):\n",
    "    token_id = greedy_tokens[i].item()\n",
    "    prob = probs[i, token_id].item()\n",
    "    print(f\"  Paso {i}: token {token_id}, prob={prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ef147",
   "metadata": {},
   "source": [
    "### 5.2 Beam Search\n",
    "\n",
    "Mantiene las $k$ mejores hipótesis en paralelo:\n",
    "\n",
    "$$\n",
    "\\text{score}(y_{1:t}) = \\sum_{i=1}^{t} \\log P(y_i | y_{<i}, x)\n",
    "$$\n",
    "\n",
    "Con normalización por longitud:\n",
    "\n",
    "$$\n",
    "\\text{score\\_norm}(y_{1:t}) = \\frac{\\text{score}(y_{1:t})}{t^\\alpha}\n",
    "$$\n",
    "\n",
    "donde $\\alpha \\approx 0.6$ evita preferencia por secuencias cortas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de beam search\n",
    "def simple_beam_search_demo(probs, beam_size=3, length_penalty=0.6):\n",
    "    \"\"\"\n",
    "    Demostración simplificada de beam search.\n",
    "    \"\"\"\n",
    "    vocab_size = probs.shape[1]\n",
    "    \n",
    "    # Inicializar con token SOS (supongamos id=1)\n",
    "    beams = [{'tokens': [1], 'score': 0.0}]\n",
    "    \n",
    "    for step in range(5):  # Generar 5 tokens\n",
    "        candidates = []\n",
    "        \n",
    "        for beam in beams:\n",
    "            # Expandir con top-k tokens\n",
    "            top_k_probs, top_k_ids = torch.topk(probs[step], beam_size)\n",
    "            \n",
    "            for prob, token_id in zip(top_k_probs, top_k_ids):\n",
    "                new_beam = {\n",
    "                    'tokens': beam['tokens'] + [token_id.item()],\n",
    "                    'score': beam['score'] + torch.log(prob).item()\n",
    "                }\n",
    "                candidates.append(new_beam)\n",
    "        \n",
    "        # Seleccionar top-k candidatos con normalización por longitud\n",
    "        candidates.sort(key=lambda x: x['score'] / (len(x['tokens']) ** length_penalty), reverse=True)\n",
    "        beams = candidates[:beam_size]\n",
    "    \n",
    "    return beams\n",
    "\n",
    "# Ejecutar demo\n",
    "beams = simple_beam_search_demo(probs, beam_size=3)\n",
    "\n",
    "print(\"Resultados de Beam Search (beam_size=3, length_penalty=0.6):\\n\")\n",
    "for i, beam in enumerate(beams):\n",
    "    score_norm = beam['score'] / (len(beam['tokens']) ** 0.6)\n",
    "    print(f\"Beam {i+1}:\")\n",
    "    print(f\"  Tokens: {beam['tokens']}\")\n",
    "    print(f\"  Score (log-prob): {beam['score']:.4f}\")\n",
    "    print(f\"  Score normalizado: {score_norm:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d045d",
   "metadata": {},
   "source": [
    "### 5.3 Top-k Sampling\n",
    "\n",
    "Muestrea del top-k tokens más probables:\n",
    "\n",
    "1. Filtrar top-k tokens\n",
    "2. Renormalizar probabilidades\n",
    "3. Muestrear de la distribución filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a45051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración de top-k sampling\n",
    "k = 10\n",
    "step_probs = probs[0]  # Primera distribución\n",
    "\n",
    "# Top-k\n",
    "top_k_probs, top_k_ids = torch.topk(step_probs, k)\n",
    "top_k_probs_norm = top_k_probs / top_k_probs.sum()\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribución original (top-20 para visualizar)\n",
    "top_20_probs, top_20_ids = torch.topk(step_probs, 20)\n",
    "axes[0].bar(range(20), top_20_probs.numpy())\n",
    "axes[0].axvline(x=k-0.5, color='red', linestyle='--', linewidth=2, label=f'Top-{k} cutoff')\n",
    "axes[0].set_xlabel('Índice (top-20)')\n",
    "axes[0].set_ylabel('Probabilidad')\n",
    "axes[0].set_title('Distribución Original (top-20 tokens)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-k renormalizado\n",
    "axes[1].bar(range(k), top_k_probs_norm.numpy())\n",
    "axes[1].set_xlabel('Índice (top-k)')\n",
    "axes[1].set_ylabel('Probabilidad (renormalizada)')\n",
    "axes[1].set_title(f'Top-{k} Sampling (renormalizado)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top-{k} sampling:\")\n",
    "print(f\"  Suma de probabilidades originales (top-{k}): {top_k_probs.sum():.4f}\")\n",
    "print(f\"  Suma de probabilidades renormalizadas: {top_k_probs_norm.sum():.4f}\")\n",
    "print(f\"  Token IDs en top-{k}: {top_k_ids.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d7efeb",
   "metadata": {},
   "source": [
    "### 5.4 Top-p (Nucleus) Sampling\n",
    "\n",
    "Muestrea del conjunto más pequeño de tokens cuya probabilidad acumulada supera $p$:\n",
    "\n",
    "$$\n",
    "V^{(p)} = \\min\\{V' \\subseteq V : \\sum_{v \\in V'} P(v) \\geq p\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94da6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración de top-p sampling\n",
    "p = 0.9\n",
    "\n",
    "# Ordenar probabilidades\n",
    "sorted_probs, sorted_ids = torch.sort(step_probs, descending=True)\n",
    "cumsum = torch.cumsum(sorted_probs, dim=0)\n",
    "\n",
    "# Encontrar núcleo\n",
    "nucleus_mask = cumsum <= p\n",
    "# Asegurar al menos un token\n",
    "nucleus_mask[0] = True\n",
    "\n",
    "nucleus_probs = sorted_probs[nucleus_mask]\n",
    "nucleus_ids = sorted_ids[nucleus_mask]\n",
    "nucleus_size = nucleus_mask.sum().item()\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(cumsum[:50].numpy(), marker='o', markersize=3, label='Probabilidad acumulada')\n",
    "plt.axhline(y=p, color='red', linestyle='--', linewidth=2, label=f'Umbral p={p}')\n",
    "plt.axvline(x=nucleus_size-0.5, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Núcleo (tamaño={nucleus_size})')\n",
    "plt.xlabel('Posición (ordenado por probabilidad)')\n",
    "plt.ylabel('Probabilidad acumulada')\n",
    "plt.title(f'Top-p (Nucleus) Sampling con p={p}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top-p (nucleus) sampling con p={p}:\")\n",
    "print(f\"  Tamaño del núcleo: {nucleus_size} tokens\")\n",
    "print(f\"  Probabilidad acumulada del núcleo: {cumsum[nucleus_size-1].item():.4f}\")\n",
    "print(f\"  Token IDs en núcleo (primeros 10): {nucleus_ids[:10].tolist()}\")\n",
    "\n",
    "# Comparación con top-k\n",
    "print(f\"\\nComparación:\")\n",
    "print(f\"  Top-k={k} siempre usa {k} tokens\")\n",
    "print(f\"  Top-p={p} usa {nucleus_size} tokens (adaptativo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18378920",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "Este notebook ha cubierto los fundamentos teóricos del Transformer:\n",
    "\n",
    "1. **Arquitectura Encoder-Decoder**: Procesamiento bidireccional del source y generación autoregresiva del target\n",
    "\n",
    "2. **Atención**: Mecanismo de atención escalada con visualización de matrices de peso\n",
    "\n",
    "3. **Máscaras**:\n",
    "   - Causal: Bloquea información futura en el decoder\n",
    "   - Padding: Ignora tokens de relleno\n",
    "\n",
    "4. **Codificaciones Posicionales**:\n",
    "   - Sinusoidal: Baseline con senos y cosenos\n",
    "   - RoPE: Rotación que preserva normas\n",
    "   - ALiBi: Sesgos lineales para mejor extrapolación\n",
    "\n",
    "5. **Decodificación**:\n",
    "   - Greedy: Rápido pero local\n",
    "   - Beam Search: Mejor calidad con búsqueda paralela\n",
    "   - Top-k: Muestreo con k tokens\n",
    "   - Top-p: Muestreo adaptativo con núcleo dinámico\n",
    "\n",
    "En los siguientes notebooks aplicaremos estos conceptos para entrenar y evaluar el modelo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
