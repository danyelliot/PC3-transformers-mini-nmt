{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cc311a",
   "metadata": {},
   "source": [
    "# Notebook 2: Entrenamiento del Transformer NMT\n",
    "\n",
    "Este notebook implementa el entrenamiento completo del modelo Transformer para traducción español-inglés.\n",
    "\n",
    "## Contenido\n",
    "1. Preparación de datos (Tatoeba)\n",
    "2. Configuración del modelo\n",
    "3. Loop de entrenamiento\n",
    "4. Monitoreo y visualización\n",
    "5. Guardado de checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd879da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.models.transformer import TransformerConfig, Transformer\n",
    "from src.data import prepare_data, Vocabulary\n",
    "from src.train import Trainer, WarmupCosineScheduler\n",
    "from src.utils import set_seed, get_device, count_parameters, save_checkpoint, load_checkpoint\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Reproducibilidad\n",
    "set_seed(42)\n",
    "\n",
    "# Detectar device\n",
    "device = get_device()\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99810e",
   "metadata": {},
   "source": [
    "## 1. Preparación de Datos\n",
    "\n",
    "Vamos a descargar y preparar el dataset Tatoeba de español-inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094815cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de datos\n",
    "data_config = {\n",
    "    'lang_pair': 'es-en',\n",
    "    'max_samples': 20000,  # Limitar para entrenamiento rápido\n",
    "    'max_length': 50,\n",
    "    'min_freq': 2,\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 4\n",
    "}\n",
    "\n",
    "print(\"Preparando datos...\")\n",
    "print(f\"  - Par de idiomas: {data_config['lang_pair']}\")\n",
    "print(f\"  - Máximo de muestras: {data_config['max_samples']}\")\n",
    "print(f\"  - Longitud máxima: {data_config['max_length']}\")\n",
    "print(f\"  - Batch size: {data_config['batch_size']}\")\n",
    "\n",
    "# Preparar datos\n",
    "data_loaders, vocab_src, vocab_tgt = prepare_data(\n",
    "    lang_pair=data_config['lang_pair'],\n",
    "    max_samples=data_config['max_samples'],\n",
    "    max_length=data_config['max_length'],\n",
    "    min_freq=data_config['min_freq'],\n",
    "    batch_size=data_config['batch_size'],\n",
    "    num_workers=data_config['num_workers'],\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_loader = data_loaders['train']\n",
    "val_loader = data_loaders['val']\n",
    "test_loader = data_loaders['test']\n",
    "\n",
    "print(f\"\\nEstadísticas de datos:\")\n",
    "print(f\"  - Vocabulario source: {len(vocab_src)} tokens\")\n",
    "print(f\"  - Vocabulario target: {len(vocab_tgt)} tokens\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")\n",
    "print(f\"  - Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ad295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionar un batch de ejemplo\n",
    "sample_batch = next(iter(train_loader))\n",
    "src_batch, tgt_batch = sample_batch\n",
    "\n",
    "print(f\"Forma del batch source: {src_batch.shape}  # (batch_size, seq_len)\")\n",
    "print(f\"Forma del batch target: {tgt_batch.shape}\")\n",
    "\n",
    "# Decodificar primera muestra\n",
    "src_sample = src_batch[0]\n",
    "tgt_sample = tgt_batch[0]\n",
    "\n",
    "src_text = ' '.join(vocab_src.decode(src_sample.tolist()))\n",
    "tgt_text = ' '.join(vocab_tgt.decode(tgt_sample.tolist()))\n",
    "\n",
    "print(f\"\\nEjemplo de par de traducción:\")\n",
    "print(f\"  Source (es): {src_text}\")\n",
    "print(f\"  Target (en): {tgt_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c001e0ad",
   "metadata": {},
   "source": [
    "## 2. Configuración del Modelo\n",
    "\n",
    "Configuramos el Transformer con los hiperparámetros óptimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c23485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo\n",
    "model_config = TransformerConfig(\n",
    "    vocab_size_src=len(vocab_src),\n",
    "    vocab_size_tgt=len(vocab_tgt),\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=data_config['max_length'],\n",
    "    pos_encoding_type='sinusoidal',  # Puede ser 'sinusoidal', 'rope', 'alibi'\n",
    "    pad_idx=vocab_src.pad_idx\n",
    ")\n",
    "\n",
    "print(\"Configuración del modelo:\")\n",
    "print(f\"  - d_model: {model_config.d_model}\")\n",
    "print(f\"  - n_heads: {model_config.n_heads}\")\n",
    "print(f\"  - encoder_layers: {model_config.num_encoder_layers}\")\n",
    "print(f\"  - decoder_layers: {model_config.num_decoder_layers}\")\n",
    "print(f\"  - dim_feedforward: {model_config.dim_feedforward}\")\n",
    "print(f\"  - dropout: {model_config.dropout}\")\n",
    "print(f\"  - pos_encoding: {model_config.pos_encoding_type}\")\n",
    "\n",
    "# Crear modelo\n",
    "model = Transformer(model_config).to(device)\n",
    "\n",
    "# Contar parámetros\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\nParámetros totales: {total_params:,}\")\n",
    "print(f\"Tamaño estimado: {total_params * 4 / 1024**2:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionar arquitectura\n",
    "print(\"\\nArquitectura del modelo:\\n\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d026e",
   "metadata": {},
   "source": [
    "## 3. Configuración de Entrenamiento\n",
    "\n",
    "Configuramos el optimizador, scheduler y criterio de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9287a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de entrenamiento\n",
    "train_config = {\n",
    "    'epochs': 30,\n",
    "    'lr': 1e-4,\n",
    "    'betas': (0.9, 0.98),\n",
    "    'eps': 1e-9,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 1000,\n",
    "    'min_lr': 1e-6,\n",
    "    'label_smoothing': 0.1,\n",
    "    'grad_clip': 1.0,\n",
    "    'use_amp': True,  # Mixed precision\n",
    "    'patience': 5,  # Early stopping\n",
    "    'checkpoint_dir': '../checkpoints',\n",
    "    'log_interval': 100\n",
    "}\n",
    "\n",
    "print(\"Configuración de entrenamiento:\")\n",
    "for key, value in train_config.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "# Crear directorio de checkpoints\n",
    "Path(train_config['checkpoint_dir']).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a7cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador\n",
    "optimizer = Adam(\n",
    "    model.parameters(),\n",
    "    lr=train_config['lr'],\n",
    "    betas=train_config['betas'],\n",
    "    eps=train_config['eps'],\n",
    "    weight_decay=train_config['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler (warmup + cosine decay)\n",
    "total_steps = len(train_loader) * train_config['epochs']\n",
    "scheduler = WarmupCosineScheduler(\n",
    "    optimizer,\n",
    "    warmup_steps=train_config['warmup_steps'],\n",
    "    total_steps=total_steps,\n",
    "    min_lr=train_config['min_lr']\n",
    ")\n",
    "\n",
    "# Criterio de pérdida (con label smoothing)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=vocab_tgt.pad_idx,\n",
    "    label_smoothing=train_config['label_smoothing']\n",
    ")\n",
    "\n",
    "# GradScaler para mixed precision\n",
    "scaler = GradScaler() if train_config['use_amp'] else None\n",
    "\n",
    "print(f\"\\nTotal steps: {total_steps:,}\")\n",
    "print(f\"Warmup steps: {train_config['warmup_steps']:,}\")\n",
    "print(f\"Steps per epoch: {len(train_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016a8587",
   "metadata": {},
   "source": [
    "### Visualización del Learning Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6663a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular learning rate schedule\n",
    "lrs = []\n",
    "for step in range(total_steps):\n",
    "    if step < train_config['warmup_steps']:\n",
    "        lr = train_config['lr'] * (step / train_config['warmup_steps'])\n",
    "    else:\n",
    "        progress = (step - train_config['warmup_steps']) / (total_steps - train_config['warmup_steps'])\n",
    "        lr = train_config['min_lr'] + (train_config['lr'] - train_config['min_lr']) * \\\n",
    "             0.5 * (1 + np.cos(np.pi * progress))\n",
    "    lrs.append(lr)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(lrs)\n",
    "plt.axvline(x=train_config['warmup_steps'], color='red', linestyle='--', \n",
    "            linewidth=2, label='Fin warmup')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule (Warmup + Cosine Decay)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"LR inicial: {lrs[0]:.2e}\")\n",
    "print(f\"LR máximo (después de warmup): {lrs[train_config['warmup_steps']]:.2e}\")\n",
    "print(f\"LR final: {lrs[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb8f7c",
   "metadata": {},
   "source": [
    "## 4. Loop de Entrenamiento\n",
    "\n",
    "Implementamos el entrenamiento con las siguientes características:\n",
    "- Mixed Precision (AMP) para velocidad\n",
    "- Gradient clipping para estabilidad\n",
    "- Early stopping\n",
    "- Checkpointing del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    scaler=scaler,\n",
    "    grad_clip=train_config['grad_clip']\n",
    ")\n",
    "\n",
    "print(\"Trainer inicializado.\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Mixed precision: {train_config['use_amp']}\")\n",
    "print(f\"  - Gradient clipping: {train_config['grad_clip']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854440d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento completo\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_ppl': [],\n",
    "    'val_ppl': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"\\nIniciando entrenamiento...\\n\")\n",
    "\n",
    "for epoch in range(train_config['epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Entrenamiento\n",
    "    train_loss = trainer.train_epoch(train_loader, epoch)\n",
    "    train_ppl = np.exp(train_loss)\n",
    "    \n",
    "    # Validación\n",
    "    val_loss = trainer.validate(val_loader)\n",
    "    val_ppl = np.exp(val_loss)\n",
    "    \n",
    "    # Learning rate actual\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Guardar historial\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['val_ppl'].append(val_ppl)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Logging\n",
    "    print(f\"Epoch {epoch+1}/{train_config['epochs']} ({epoch_time:.1f}s) - \"\n",
    "          f\"Train Loss: {train_loss:.4f} (PPL: {train_ppl:.2f}) - \"\n",
    "          f\"Val Loss: {val_loss:.4f} (PPL: {val_ppl:.2f}) - \"\n",
    "          f\"LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Guardar mejor modelo\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        checkpoint_path = Path(train_config['checkpoint_dir']) / 'best_model.pt'\n",
    "        save_checkpoint(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            step=trainer.global_step,\n",
    "            loss=val_loss,\n",
    "            config=model_config,\n",
    "            path=checkpoint_path\n",
    "        )\n",
    "        print(f\"  -> Mejor modelo guardado (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  -> No mejora ({patience_counter}/{train_config['patience']})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= train_config['patience']:\n",
    "        print(f\"\\nEarly stopping después de {epoch+1} epochs.\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")\n",
    "print(f\"Mejor val_loss: {best_val_loss:.4f} (PPL: {np.exp(best_val_loss):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597effc5",
   "metadata": {},
   "source": [
    "## 5. Visualización de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas de pérdida\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o', markersize=4)\n",
    "axes[0].plot(history['val_loss'], label='Validation', marker='s', markersize=4)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "axes[1].plot(history['train_ppl'], label='Train', marker='o', markersize=4)\n",
    "axes[1].plot(history['val_ppl'], label='Validation', marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Training and Validation Perplexity')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history['lr'], marker='o', markersize=4, color='green')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Gráficas guardadas en ../outputs/training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas finales\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTADÍSTICAS FINALES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nÉpocas entrenadas: {len(history['train_loss'])}\")\n",
    "print(f\"\\nPérdida:\")\n",
    "print(f\"  - Mejor train loss: {min(history['train_loss']):.4f}\")\n",
    "print(f\"  - Mejor val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  - Pérdida final train: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  - Pérdida final val: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"\\nPerplejidad:\")\n",
    "print(f\"  - Mejor train PPL: {min(history['train_ppl']):.2f}\")\n",
    "print(f\"  - Mejor val PPL: {min(history['val_ppl']):.2f}\")\n",
    "print(f\"  - PPL final train: {history['train_ppl'][-1]:.2f}\")\n",
    "print(f\"  - PPL final val: {history['val_ppl'][-1]:.2f}\")\n",
    "print(f\"\\nLearning rate:\")\n",
    "print(f\"  - LR inicial: {history['lr'][0]:.2e}\")\n",
    "print(f\"  - LR final: {history['lr'][-1]:.2e}\")\n",
    "print(f\"\\nCheckpoint:\")\n",
    "print(f\"  - Mejor modelo: {train_config['checkpoint_dir']}/best_model.pt\")\n",
    "print(f\"  - Val loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - Val PPL: {np.exp(best_val_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e50979",
   "metadata": {},
   "source": [
    "## 6. Guardar Vocabularios y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f7bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar vocabularios\n",
    "vocab_src.save('../data/processed/vocab_src.pkl')\n",
    "vocab_tgt.save('../data/processed/vocab_tgt.pkl')\n",
    "\n",
    "print(\"Vocabularios guardados:\")\n",
    "print(f\"  - Source: ../data/processed/vocab_src.pkl\")\n",
    "print(f\"  - Target: ../data/processed/vocab_tgt.pkl\")\n",
    "\n",
    "# Guardar configuración\n",
    "import json\n",
    "\n",
    "full_config = {\n",
    "    'model': {\n",
    "        'vocab_size_src': len(vocab_src),\n",
    "        'vocab_size_tgt': len(vocab_tgt),\n",
    "        'd_model': model_config.d_model,\n",
    "        'n_heads': model_config.n_heads,\n",
    "        'num_encoder_layers': model_config.num_encoder_layers,\n",
    "        'num_decoder_layers': model_config.num_decoder_layers,\n",
    "        'dim_feedforward': model_config.dim_feedforward,\n",
    "        'dropout': model_config.dropout,\n",
    "        'max_seq_len': model_config.max_seq_len,\n",
    "        'pos_encoding_type': model_config.pos_encoding_type\n",
    "    },\n",
    "    'data': data_config,\n",
    "    'training': train_config,\n",
    "    'results': {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_ppl': np.exp(best_val_loss),\n",
    "        'total_epochs': len(history['train_loss']),\n",
    "        'total_params': total_params\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertir objetos Path a string\n",
    "full_config['training']['checkpoint_dir'] = str(full_config['training']['checkpoint_dir'])\n",
    "\n",
    "with open('../outputs/training_config.json', 'w') as f:\n",
    "    json.dump(full_config, f, indent=2)\n",
    "\n",
    "print(\"\\nConfiguración guardada en ../outputs/training_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3abff",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. Preparado el dataset Tatoeba español-inglés\n",
    "2. Configurado el modelo Transformer con 4 capas encoder/decoder\n",
    "3. Implementado entrenamiento con:\n",
    "   - Mixed Precision (AMP) para eficiencia\n",
    "   - Warmup + Cosine LR schedule\n",
    "   - Label smoothing\n",
    "   - Gradient clipping\n",
    "   - Early stopping\n",
    "4. Visualizado curvas de entrenamiento\n",
    "5. Guardado del mejor modelo y configuración\n",
    "\n",
    "El modelo está listo para evaluación y generación de traducciones en los siguientes notebooks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
